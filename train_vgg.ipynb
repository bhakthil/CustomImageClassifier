{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# USAGE\n",
    "# python train_vgg.py --dataset animals --model output/smallvggnet.model --label-bin output/smallvggnet_lb.pickle --plot output/smallvggnet_plot.png\n",
    "\n",
    "# set the matplotlib backend so figures can be saved in the background\n",
    "\n",
    "#matplotlib.use(\"Agg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# import the necessary packages\n",
    "from pyimagesearch.smallvggnet import SmallVGGNet\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.optimizers import SGD\n",
    "from imutils import paths\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import argparse\n",
    "import random\n",
    "import pickle\n",
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct the argument parser and parse the arguments\n",
    "#ap = argparse.ArgumentParser()\n",
    "#ap.add_argument(\"--dataset\", type=str, default=\"animals\",help=\"path to input dataset of images\")\n",
    "#ap.add_argument(\"-m\", \"--model\", type=str, default=\"output/vgg_nn.model\", help=\"path to output trained model\")\n",
    "#ap.add_argument(\"-l\", \"--label-bin\", type=str, default=\"output/vgg_nn_lb.pickle\", help=\"path to output label binarizer\")\n",
    "#ap.add_argument(\"-p\", \"--plot\", type=str, default=\"output/vgg_nn_plot.png\", help=\"path to output accuracy/loss plot\")\n",
    "#args = ap.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct the argument parser and parse the arguments\n",
    "args = {\n",
    "  \"dataset\": \"animals\", # path to input dataset of images\n",
    "  \"model\": \"output/vgg_nn.model\",    # path to output trained model\n",
    "  \"label-bin\": \"output/vgg_nn_lb.pickle\",# path to output label binarizer\n",
    "  \"plot\":\"output/vgg_nn_plot.png\"       # path to output accuracy/loss plot\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading images...\n"
     ]
    }
   ],
   "source": [
    "# initialize the data and labels\n",
    "print(\"[INFO] loading images...\")\n",
    "data = []\n",
    "labels = []\n",
    "\n",
    "# grab the image paths and randomly shuffle them\n",
    "imagePaths = sorted(list(paths.list_images(args[\"dataset\"])))\n",
    "random.seed(42)\n",
    "random.shuffle(imagePaths)\n",
    "\n",
    "# loop over the input images\n",
    "for imagePath in imagePaths:\n",
    "    # load the image, resize it to 64x64 pixels (the required input spatial dimensions of SmallVGGNet), and store the image in the\n",
    "    # data list\n",
    "    image = cv2.imread(imagePath)\n",
    "    image = cv2.resize(image, (64, 64))\n",
    "    data.append(image)\n",
    "\n",
    "    # extract the class label from the image path and update the\n",
    "    # labels list\n",
    "    label = imagePath.split(os.path.sep)[-2]\n",
    "    labels.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale the raw pixel intensities to the range [0, 1]\n",
    "data = np.array(data, dtype=\"float\") / 255.0\n",
    "labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# partition the data into training and testing splits using 75% of\n",
    "# the data for training and the remaining 25% for testing\n",
    "(trainX, testX, trainY, testY) = train_test_split(data,labels, test_size=0.25, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " convert the labels from integers to vectors (for 2-class, binary\n",
    " classification you should use Keras' to_categorical function\n",
    " instead as the scikit-learn's LabelBinarizer will not return a\n",
    " vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "lb = LabelBinarizer()\n",
    "trainY = lb.fit_transform(trainY)\n",
    "testY = lb.transform(testY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct the image generator for data augmentation\n",
    "aug = ImageDataGenerator(rotation_range=30, width_shift_range=0.1, \n",
    "                         height_shift_range=0.1, shear_range=0.2, zoom_range=0.2,\n",
    "                         horizontal_flip=True, fill_mode=\"nearest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Anaconda3\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Anaconda3\\envs\\py36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "# initialize our VGG-like Convolutional Neural Network\n",
    "model = SmallVGGNet.build(width=64, height=64, depth=3, classes=len(lb.classes_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize our initial learning rate, # of epochs to train for,\n",
    "# and batch size\n",
    "INIT_LR = 0.01\n",
    "EPOCHS = 75\n",
    "BS = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] training network...\n"
     ]
    }
   ],
   "source": [
    "# initialize the model and optimizer (you'll want to use binary_crossentropy for 2-class classification)\n",
    "print(\"[INFO] training network...\")\n",
    "opt = SGD(lr=INIT_LR, decay=INIT_LR / EPOCHS)\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Anaconda3\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/75\n",
      "70/70 [==============================] - 66s 938ms/step - loss: 1.3574 - acc: 0.5259 - val_loss: 1.2133 - val_acc: 0.5773\n",
      "Epoch 2/75\n",
      "70/70 [==============================] - 62s 886ms/step - loss: 1.0585 - acc: 0.5810 - val_loss: 0.8417 - val_acc: 0.6453\n",
      "Epoch 3/75\n",
      "70/70 [==============================] - 61s 873ms/step - loss: 0.9070 - acc: 0.6175 - val_loss: 1.2626 - val_acc: 0.4600\n",
      "Epoch 4/75\n",
      "70/70 [==============================] - 62s 883ms/step - loss: 0.8315 - acc: 0.6267 - val_loss: 0.7422 - val_acc: 0.6640\n",
      "Epoch 5/75\n",
      "70/70 [==============================] - 61s 870ms/step - loss: 0.7849 - acc: 0.6339 - val_loss: 0.7370 - val_acc: 0.6573\n",
      "Epoch 6/75\n",
      "70/70 [==============================] - 60s 859ms/step - loss: 0.7495 - acc: 0.6465 - val_loss: 0.8271 - val_acc: 0.6213\n",
      "Epoch 7/75\n",
      "70/70 [==============================] - 60s 855ms/step - loss: 0.7348 - acc: 0.6545 - val_loss: 0.6462 - val_acc: 0.6827\n",
      "Epoch 8/75\n",
      "70/70 [==============================] - 61s 870ms/step - loss: 0.6880 - acc: 0.6654 - val_loss: 0.6530 - val_acc: 0.6987\n",
      "Epoch 9/75\n",
      "70/70 [==============================] - 62s 882ms/step - loss: 0.6744 - acc: 0.6764 - val_loss: 0.8180 - val_acc: 0.6453\n",
      "Epoch 10/75\n",
      "70/70 [==============================] - 62s 887ms/step - loss: 0.6848 - acc: 0.6778 - val_loss: 0.6560 - val_acc: 0.6960\n",
      "Epoch 11/75\n",
      "70/70 [==============================] - 61s 864ms/step - loss: 0.6632 - acc: 0.6867 - val_loss: 0.6435 - val_acc: 0.7240\n",
      "Epoch 12/75\n",
      "70/70 [==============================] - 61s 873ms/step - loss: 0.6422 - acc: 0.6840 - val_loss: 0.6737 - val_acc: 0.6640\n",
      "Epoch 13/75\n",
      "70/70 [==============================] - 61s 872ms/step - loss: 0.6389 - acc: 0.6961 - val_loss: 0.6006 - val_acc: 0.7200\n",
      "Epoch 14/75\n",
      "70/70 [==============================] - 61s 877ms/step - loss: 0.6270 - acc: 0.6970 - val_loss: 0.6287 - val_acc: 0.7053\n",
      "Epoch 15/75\n",
      "70/70 [==============================] - 63s 907ms/step - loss: 0.6157 - acc: 0.6984 - val_loss: 0.6800 - val_acc: 0.6867\n",
      "Epoch 16/75\n",
      "70/70 [==============================] - 62s 892ms/step - loss: 0.6288 - acc: 0.6974 - val_loss: 0.5957 - val_acc: 0.7240\n",
      "Epoch 17/75\n",
      "70/70 [==============================] - 66s 937ms/step - loss: 0.5994 - acc: 0.7150 - val_loss: 0.6866 - val_acc: 0.7013\n",
      "Epoch 18/75\n",
      "70/70 [==============================] - 66s 940ms/step - loss: 0.6161 - acc: 0.7081 - val_loss: 0.5578 - val_acc: 0.7400\n",
      "Epoch 19/75\n",
      "70/70 [==============================] - 63s 904ms/step - loss: 0.5902 - acc: 0.7031 - val_loss: 0.7662 - val_acc: 0.6853\n",
      "Epoch 20/75\n",
      "70/70 [==============================] - 62s 887ms/step - loss: 0.5764 - acc: 0.7302 - val_loss: 0.8049 - val_acc: 0.6600\n",
      "Epoch 21/75\n",
      "70/70 [==============================] - 63s 904ms/step - loss: 0.5869 - acc: 0.7207 - val_loss: 0.6060 - val_acc: 0.7307\n",
      "Epoch 22/75\n",
      "70/70 [==============================] - 61s 877ms/step - loss: 0.5858 - acc: 0.7233 - val_loss: 0.8333 - val_acc: 0.6280\n",
      "Epoch 23/75\n",
      "70/70 [==============================] - 61s 869ms/step - loss: 0.5881 - acc: 0.7212 - val_loss: 0.7824 - val_acc: 0.6587\n",
      "Epoch 24/75\n",
      "70/70 [==============================] - 61s 877ms/step - loss: 0.5639 - acc: 0.7274 - val_loss: 0.5872 - val_acc: 0.7267\n",
      "Epoch 25/75\n",
      "70/70 [==============================] - 61s 877ms/step - loss: 0.5764 - acc: 0.7305 - val_loss: 0.5972 - val_acc: 0.7413\n",
      "Epoch 26/75\n",
      "70/70 [==============================] - 62s 881ms/step - loss: 0.5837 - acc: 0.7312 - val_loss: 0.7554 - val_acc: 0.7040\n",
      "Epoch 27/75\n",
      "70/70 [==============================] - 61s 874ms/step - loss: 0.5544 - acc: 0.7260 - val_loss: 0.6636 - val_acc: 0.7227\n",
      "Epoch 28/75\n",
      "70/70 [==============================] - 62s 888ms/step - loss: 0.5704 - acc: 0.7340 - val_loss: 0.5718 - val_acc: 0.7467\n",
      "Epoch 29/75\n",
      "70/70 [==============================] - 62s 893ms/step - loss: 0.5567 - acc: 0.7409 - val_loss: 0.5852 - val_acc: 0.7440\n",
      "Epoch 30/75\n",
      "70/70 [==============================] - 62s 892ms/step - loss: 0.5524 - acc: 0.7386 - val_loss: 0.6314 - val_acc: 0.7307\n",
      "Epoch 31/75\n",
      "70/70 [==============================] - 63s 906ms/step - loss: 0.5310 - acc: 0.7510 - val_loss: 0.5730 - val_acc: 0.7480\n",
      "Epoch 32/75\n",
      "70/70 [==============================] - 62s 879ms/step - loss: 0.5359 - acc: 0.7534 - val_loss: 0.5558 - val_acc: 0.7600\n",
      "Epoch 33/75\n",
      "70/70 [==============================] - 61s 878ms/step - loss: 0.5383 - acc: 0.7458 - val_loss: 0.6595 - val_acc: 0.7320\n",
      "Epoch 34/75\n",
      "70/70 [==============================] - 62s 891ms/step - loss: 0.5282 - acc: 0.7431 - val_loss: 0.5629 - val_acc: 0.7653\n",
      "Epoch 35/75\n",
      "70/70 [==============================] - 115s 2s/step - loss: 0.5412 - acc: 0.7509 - val_loss: 0.5775 - val_acc: 0.7507\n",
      "Epoch 36/75\n",
      "70/70 [==============================] - 109s 2s/step - loss: 0.5252 - acc: 0.7657 - val_loss: 0.5510 - val_acc: 0.7667\n",
      "Epoch 37/75\n",
      "70/70 [==============================] - 60s 864ms/step - loss: 0.5229 - acc: 0.7497 - val_loss: 0.6176 - val_acc: 0.7440\n",
      "Epoch 38/75\n",
      "70/70 [==============================] - 61s 869ms/step - loss: 0.5194 - acc: 0.7543 - val_loss: 0.5581 - val_acc: 0.7480\n",
      "Epoch 39/75\n",
      "70/70 [==============================] - 62s 883ms/step - loss: 0.5038 - acc: 0.7583 - val_loss: 0.5544 - val_acc: 0.7760\n",
      "Epoch 40/75\n",
      "70/70 [==============================] - 62s 887ms/step - loss: 0.5272 - acc: 0.7500 - val_loss: 0.5591 - val_acc: 0.7573\n",
      "Epoch 41/75\n",
      "70/70 [==============================] - 62s 887ms/step - loss: 0.4870 - acc: 0.7655 - val_loss: 0.5401 - val_acc: 0.7720\n",
      "Epoch 42/75\n",
      "70/70 [==============================] - 62s 883ms/step - loss: 0.5122 - acc: 0.7628 - val_loss: 0.5448 - val_acc: 0.7693\n",
      "Epoch 43/75\n",
      "70/70 [==============================] - 63s 904ms/step - loss: 0.5034 - acc: 0.7727 - val_loss: 0.6503 - val_acc: 0.7293\n",
      "Epoch 44/75\n",
      "70/70 [==============================] - 64s 912ms/step - loss: 0.4941 - acc: 0.7775 - val_loss: 0.6673 - val_acc: 0.7187\n",
      "Epoch 45/75\n",
      "70/70 [==============================] - 60s 863ms/step - loss: 0.5117 - acc: 0.7713 - val_loss: 0.6122 - val_acc: 0.7240\n",
      "Epoch 46/75\n",
      "70/70 [==============================] - 60s 860ms/step - loss: 0.4880 - acc: 0.7848 - val_loss: 0.7142 - val_acc: 0.7187\n",
      "Epoch 47/75\n",
      "70/70 [==============================] - 60s 861ms/step - loss: 0.5102 - acc: 0.7642 - val_loss: 0.5821 - val_acc: 0.7440\n",
      "Epoch 48/75\n",
      "70/70 [==============================] - 61s 873ms/step - loss: 0.5005 - acc: 0.7663 - val_loss: 0.5718 - val_acc: 0.7427\n",
      "Epoch 49/75\n",
      "70/70 [==============================] - 60s 860ms/step - loss: 0.4849 - acc: 0.7767 - val_loss: 0.5514 - val_acc: 0.7600\n",
      "Epoch 50/75\n",
      "70/70 [==============================] - 60s 863ms/step - loss: 0.4844 - acc: 0.7833 - val_loss: 0.5698 - val_acc: 0.7640\n",
      "Epoch 51/75\n",
      "70/70 [==============================] - 60s 858ms/step - loss: 0.4867 - acc: 0.7761 - val_loss: 0.5081 - val_acc: 0.7853\n",
      "Epoch 52/75\n",
      "70/70 [==============================] - 60s 860ms/step - loss: 0.4522 - acc: 0.8020 - val_loss: 0.6226 - val_acc: 0.7413\n",
      "Epoch 53/75\n",
      "70/70 [==============================] - 60s 858ms/step - loss: 0.4664 - acc: 0.7811 - val_loss: 0.6834 - val_acc: 0.7453\n",
      "Epoch 54/75\n",
      "70/70 [==============================] - 60s 858ms/step - loss: 0.4964 - acc: 0.7782 - val_loss: 0.5143 - val_acc: 0.7813\n",
      "Epoch 55/75\n",
      "70/70 [==============================] - 61s 865ms/step - loss: 0.4845 - acc: 0.7837 - val_loss: 0.6122 - val_acc: 0.7520\n",
      "Epoch 56/75\n",
      "70/70 [==============================] - 60s 864ms/step - loss: 0.4726 - acc: 0.7801 - val_loss: 0.8199 - val_acc: 0.7053\n",
      "Epoch 57/75\n",
      "70/70 [==============================] - 61s 876ms/step - loss: 0.4613 - acc: 0.7973 - val_loss: 0.8661 - val_acc: 0.6947\n",
      "Epoch 58/75\n",
      "70/70 [==============================] - 60s 863ms/step - loss: 0.4566 - acc: 0.7922 - val_loss: 0.6328 - val_acc: 0.7293\n",
      "Epoch 59/75\n",
      "70/70 [==============================] - 61s 865ms/step - loss: 0.4787 - acc: 0.7901 - val_loss: 0.5958 - val_acc: 0.7587\n",
      "Epoch 60/75\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70/70 [==============================] - 60s 861ms/step - loss: 0.4626 - acc: 0.7936 - val_loss: 1.1067 - val_acc: 0.6240\n",
      "Epoch 61/75\n",
      "70/70 [==============================] - 60s 856ms/step - loss: 0.4694 - acc: 0.7852 - val_loss: 0.5495 - val_acc: 0.7760\n",
      "Epoch 62/75\n",
      "70/70 [==============================] - 60s 857ms/step - loss: 0.4455 - acc: 0.7944 - val_loss: 0.5434 - val_acc: 0.7667\n",
      "Epoch 63/75\n",
      "70/70 [==============================] - 60s 860ms/step - loss: 0.4433 - acc: 0.7970 - val_loss: 0.5508 - val_acc: 0.7680\n",
      "Epoch 64/75\n",
      "70/70 [==============================] - 60s 855ms/step - loss: 0.4467 - acc: 0.7986 - val_loss: 0.5010 - val_acc: 0.8000\n",
      "Epoch 65/75\n",
      "70/70 [==============================] - 60s 855ms/step - loss: 0.4912 - acc: 0.7808 - val_loss: 1.8212 - val_acc: 0.5053\n",
      "Epoch 66/75\n",
      "70/70 [==============================] - 60s 860ms/step - loss: 0.4519 - acc: 0.7907 - val_loss: 0.5430 - val_acc: 0.7613\n",
      "Epoch 67/75\n",
      "70/70 [==============================] - 60s 855ms/step - loss: 0.4537 - acc: 0.7962 - val_loss: 0.5000 - val_acc: 0.7920\n",
      "Epoch 68/75\n",
      "70/70 [==============================] - 60s 858ms/step - loss: 0.4230 - acc: 0.8160 - val_loss: 0.5183 - val_acc: 0.7747\n",
      "Epoch 69/75\n",
      "70/70 [==============================] - 61s 876ms/step - loss: 0.4308 - acc: 0.8106 - val_loss: 0.5288 - val_acc: 0.7840\n",
      "Epoch 70/75\n",
      "70/70 [==============================] - 62s 890ms/step - loss: 0.4384 - acc: 0.8038 - val_loss: 0.5041 - val_acc: 0.7867\n",
      "Epoch 71/75\n",
      "70/70 [==============================] - 65s 928ms/step - loss: 0.4284 - acc: 0.8192 - val_loss: 0.4839 - val_acc: 0.7880\n",
      "Epoch 72/75\n",
      "70/70 [==============================] - 63s 895ms/step - loss: 0.4242 - acc: 0.8125 - val_loss: 0.5460 - val_acc: 0.7733\n",
      "Epoch 73/75\n",
      "70/70 [==============================] - 62s 880ms/step - loss: 0.4143 - acc: 0.8146 - val_loss: 0.6189 - val_acc: 0.7547\n",
      "Epoch 74/75\n",
      "70/70 [==============================] - 61s 874ms/step - loss: 0.4315 - acc: 0.8062 - val_loss: 0.4944 - val_acc: 0.7760\n",
      "Epoch 75/75\n",
      "70/70 [==============================] - 62s 883ms/step - loss: 0.4693 - acc: 0.7809 - val_loss: 0.5917 - val_acc: 0.7400\n",
      "Wall time: 1h 18min 33s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# train the network\n",
    "H = model.fit_generator(aug.flow(trainX, trainY, batch_size=BS),\n",
    "                        validation_data=(testX, testY), \n",
    "                        steps_per_epoch=len(trainX) // BS,\n",
    "                        epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So it took more than 1hr to train (Wall time: 1h 6min 7s) with validation accuracyof 98%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] evaluating network...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        cats       0.67      0.76      0.71       236\n",
      "        dogs       0.71      0.47      0.56       236\n",
      "       panda       0.81      0.96      0.88       278\n",
      "\n",
      "   micro avg       0.74      0.74      0.74       750\n",
      "   macro avg       0.73      0.73      0.72       750\n",
      "weighted avg       0.73      0.74      0.73       750\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# evaluate the network\n",
    "print(\"[INFO] evaluating network...\")\n",
    "predictions = model.predict(testX, batch_size=32)\n",
    "print(classification_report(testY.argmax(axis=1), predictions.argmax(axis=1), target_names=lb.classes_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see an overall accuracy of 74% which is better than simple NN. It looks like that panda class has the best accuracy while dogss class has the second best. let's try to see where we get the best accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'H' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-add3146a05bf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstyle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"ggplot\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mH\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"loss\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"train_loss\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mH\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"val_loss\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"val_loss\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mH\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"acc\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"train_acc\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'H' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the training loss and accuracy\n",
    "N = np.arange(0, EPOCHS)\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure()\n",
    "plt.plot(N, H.history[\"loss\"], label=\"train_loss\")\n",
    "plt.plot(N, H.history[\"val_loss\"], label=\"val_loss\")\n",
    "plt.plot(N, H.history[\"acc\"], label=\"train_acc\")\n",
    "plt.plot(N, H.history[\"val_acc\"], label=\"val_acc\")\n",
    "plt.title(\"Training Loss and Accuracy (SmallVGGNet)\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss/Accuracy\")\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see validation accuracy drops around epoc 65. but, don't see any overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.savefig(args[\"plot\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] serializing network and label binarizer...\n"
     ]
    }
   ],
   "source": [
    "# save the model and label binarizer to disk\n",
    "print(\"[INFO] serializing network and label binarizer...\")\n",
    "model.save(args[\"model\"])\n",
    "f = open(args[\"label-bin\"], \"wb\")\n",
    "f.write(pickle.dumps(lb))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "py36",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
